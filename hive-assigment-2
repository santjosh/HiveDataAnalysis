Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
No because any select query that is executed on Hive which does not include group by, joins, aggregate functions, or complex constraints then reducer is not called.

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
If hive uses a embedded metastore, then it is not possible to have many sessions. But it can allow multiple sesssion if there is local metastore.

Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Partitioning can be created on the month column, so that the aggregation tasks are faster.

How can you add a new partition for the month December in the above partitioned table?
LOAD DATA LOCAL INPATH 'path' into table table_name partition(month='December');

I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
By setting these prop 
hive> SET hive.exec.dynamic.partition=true;
hive> SET hive.exec.dynamic.partition.mode=non-strict;




Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
By setting the SerDe properties while creating the table as shown below:

CREATE EXTERNAL TABLE test1 (
first_name string,
 last_name string,
 email string,
 gender string,
 ip_address string) 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 
WITH SERDEPROPERTIES ("separatorChar" = ",", "escapeChar" = "\\") 
LOCATION
org.apache.hadoop.hive.serde2.OpenCSVSerde 


Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Can create an external and point the location the to directory in HDFS like below
CREATE [EXTERNAL] TABLE db.tbl(
id int, name strig, e-mail string, country string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY (delimiter)
LINES TERMINATED BY '\n'
LOCATION '/path/to/'


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;


The following statement failed to execute. What can be the cause?

All the file in the given directory are not of same format or schema.

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Yes














Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
hive> create table mytable_csv1(
    > given_date string,
    > given_time string,
    > CO_GT_ string,
    > PT08_S1_CO_ int,
    > NMHC_GT_ int,
    > C6H6_GT_ string,
    > PT08_S2_NMHC_ int,
    > NOx_GT_ int,
    > PT08_S3_NOx_ int,
    > NO2_GT_ int,
    > PT08_S4_NO2_ int,
    > PT08_S5_O3_ int ,
    > T string,
    > RH string,
    > AH string 
    > )
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' 
    > WITH SERDEPROPERTIES ("separatorChar" = ";", "escapeChar" = "\\")
    > LOCATION '/data/airquality/'
    > tblproperties("skip.header.line.count"="1");
OK
Time taken: 0.184 seconds
2. try to place a data into table location
3. Perform a select operation . 

hive> select * from mytable_csv1 limit 10;
OK
mytable_csv1.given_date	mytable_csv1.given_time	mytable_csv1.co_gt_	mytable_csv1.pt08_s1_co_	mytable_csv1.nmhc_gt_	mytable_csv1.c6h6_gt_	mytable_csv1.pt08_s2_nmhc_	mytable_csv1.nox_gt_	mytable_csv1.pt08_s3_nox_	mytable_csv1.no2_gt_	mytable_csv1.pt08_s4_no2_	mytable_csv1.pt08_s5_o3_	mytable_csv1.t	mytable_csv1.rh	mytable_csv1.ah
10/03/2004	18.00.00	2,6	1360	150	11,9	1046	166	1056	113	1692	1268	13,6	48,9	0,7578
10/03/2004	19.00.00	2	1292	112	9,4	955	103	1174	92	1559	972	13,3	47,7	0,7255
10/03/2004	20.00.00	2,2	1402	88	9,0	939	131	1140	114	1555	1074	11,9	54,0	0,7502
10/03/2004	21.00.00	2,2	1376	80	9,2	948	172	1092	122	1584	1203	11,0	60,0	0,7867
10/03/2004	22.00.00	1,6	1272	51	6,5	836	131	1205	116	1490	1110	11,2	59,6	0,7888
10/03/2004	23.00.00	1,2	1197	38	4,7	750	89	1337	96	1393	949	11,2	59,2	0,7848
11/03/2004	00.00.00	1,2	1185	31	3,6	690	62	1462	77	1333	733	11,3	56,8	0,7603
11/03/2004	01.00.00	1	1136	31	3,3	672	62	1453	76	1333	730	10,7	60,0	0,7702
11/03/2004	02.00.00	0,9	1094	24	2,3	609	45	1579	60	1276	620	10,7	59,7	0,7648
11/03/2004	03.00.00	0,6	1010	19	1,7	561	-200	1705	-200	1235	501	10,3	60,2	0,7517
Time taken: 0.837 seconds, Fetched: 10 row(s)

4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
hive> with ctea as (select year(from_unixtime(unix_timestamp(given_date,'dd/M/yyyy'),'yyyy-MM-dd')) as year from mytable_csv)
    > select year, count(*) from ctea group by year;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230225094151_a3c35a25-dd2e-4a19-a107-8b96f869752a
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:41:54,409 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local770886678_0004
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 4792310 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year	_c1
NULL	115
2004	7110
2005	2247
Time taken: 2.487 seconds, Fetched: 3 row(s)
7. Perform filter operation at least 5 kinds of filter examples . 

8. show and example of regex operation
9. alter table operation 
10 . drop table operation
12 . order by operation . 
hive> with ctea as (select year(from_unixtime(unix_timestamp(given_date,'dd/M/yyyy'),'yyyy-MM-dd')) as year from mytable_csv)
    > select year, count(*) as num from ctea group by year order by num;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230225094426_d1b7c004-e78f-4d1e-983e-b8157ac99a2c
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:44:29,703 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local633133002_0005
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:44:31,145 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local1389527974_0006
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 6362440 HDFS Write: 0 SUCCESS
Stage-Stage-2:  HDFS Read: 6362440 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year	num
NULL	115
2005	2247
2004	7110
Time taken: 4.298 seconds, Fetched: 3 row(s)
13 . where clause operations you have to perform .
hive> with ctea as (select year(from_unixtime(unix_timestamp(given_date,'dd/M/yyyy'),'yyyy-MM-dd')) as year from mytable_csv)
    > select count(*) as num from ctea where year>2004;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230225094551_58bfede8-2f12-4d3c-aa31-3155b4945fe6
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:45:53,820 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local639532634_0007
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 7932570 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
num
2247
Time taken: 2.21 seconds, Fetched: 1 row(s) 
14 . sorting operation you have to perform .
hive> with ctea as (select year(from_unixtime(unix_timestamp(given_date,'dd/M/yyyy'),'yyyy-MM-dd')) as year from mytable_csv)
    > select year, count(*) as num from ctea group by year order by num desc;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230225094636_3e9be230-4bba-4c2d-b9ad-acf3fda4850c
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:46:39,343 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local871346117_0008
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:46:40,762 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local1140555899_0009
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 9502700 HDFS Write: 0 SUCCESS
Stage-Stage-2:  HDFS Read: 9502700 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year	num
2004	7110
2005	2247
NULL	115
Time taken: 3.922 seconds, Fetched: 3 row(s)
hive> 

15 . distinct operation you have to perform . 
hive> with ctea as (select year(from_unixtime(unix_timestamp(given_date,'dd/M/yyyy'),'yyyy-MM-dd')) as year from mytable_csv)
    > select distinct(year) from ctea;
WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
Query ID = root_20230225094747_05567f7b-8b9d-4519-8b94-e54e614643c1
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Job running in-process (local Hadoop)
2023-02-25 09:47:49,534 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_local1071981373_0010
MapReduce Jobs Launched: 
Stage-Stage-1:  HDFS Read: 11072830 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 0 msec
OK
year
NULL
2004
2005
Time taken: 2.476 seconds, Fetched: 3 row(s)
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 






hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
